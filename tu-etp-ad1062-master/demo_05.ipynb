{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework of Ch5. Long Short-Term Memory\n",
    "----\n",
    "This is the homework of TU-ETP-AD1062 Machine Learning Fundamentals.\n",
    "\n",
    "For more information, please refer to:\n",
    "https://sites.google.com/view/tu-ad1062-mlfundamentals/\n",
    "\n",
    "For original dataset information, please visit:\n",
    "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn.model_selection\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Pre-processing SMS Spam Collection Dataset \n",
    "----\n",
    "The code here demonstrate how to load the `SMS Spam Collection` Dataset. Here also includes some pre-processing steps:\n",
    "1. Load SMS Spam Collection Dataset\n",
    "2. Define the following parameters:\n",
    "    * Vocabury size for one-hot-encodding (`vocab_size`)\n",
    "    * Maximum Length for each sentence (`max_len`)\n",
    "    * Maximum features, which is the output dimension of Word embedding (`max_features`)\n",
    "    * Batch size for training (`batch_size`)\n",
    "3. Conduct one-hot-encodding: Use `keras.preprocessing.text.one_hot`, which helps you\n",
    "    * Segment the sentence into the word array\n",
    "    * Encode each word via One-hot-encodding  \n",
    "    Notice that you should encoding your training data and test data together\n",
    "4. Pre-pend 0 to the one-hot-encodded sentence into fix length: Use `keras.preprocessing.sequence.pad_sequences`\n",
    "\n",
    "> **Your task**:  \n",
    "> Complete step 2,3 and 4 mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1. Load SMS Spam Collection Dataset \n",
    "f_train = open('uciml_sms_spam_train_tu-etp-ad1062-hw5.pickle', 'rb')\n",
    "(X_train, y_train) = pickle.load(f_train)\n",
    "f_train.close()\n",
    "\n",
    "f_test = open('uciml_sms_spam_test_tu-etp-ad1062-hw5.pickle', 'rb')\n",
    "X_test = pickle.load(f_test)\n",
    "f_test.close()\n",
    "\n",
    "print(\"Step1. Result of original sentence:\")\n",
    "print(\"%s\\n\" % X_train[0])\n",
    "\n",
    "# Step 2. Define the following parameters:\n",
    "#  - Vocabury size for one-hot-encodding (vocab_size)\n",
    "#  - Maximum Length for each sentence (max_len)\n",
    "#  - Maximum features, which is the output dimension of Word embedding (max_features)\n",
    "#  - Batch size for training (batch_size)\n",
    "# \n",
    "# Now it's your turn!: Adjust your parameters to maximize the performance\n",
    "# ----------------------------------------------------------------\n",
    "vocab_size = 1024\n",
    "max_len = 128\n",
    "max_features = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Step 3. One-hot-encodding\n",
    "# \n",
    "# Now it's your turn!:\n",
    "#     Use keras.preprocessing.text.one_hot to conduct one-hot-encodding\n",
    "#     Remember to specify the parameter n with the vocab_size assigned before\n",
    "# ----------------------------------------------------------------\n",
    "# ...\n",
    "\n",
    "print(\"Step3. Result of one_hot:\")\n",
    "print(\"%s\\n\" % X_train[0])\n",
    "\n",
    "# Step 4. Pre-pend 0 to the one-hot-encodded sentence into fix length\n",
    "#\n",
    "# Now it's your turn!:\n",
    "#     Use keras.preprocessing.sequence.pad_sequences\n",
    "#     Remember to specify the parameter maxlen with the max_len assigned before\n",
    "# ----------------------------------------------------------------\n",
    "# ...\n",
    "\n",
    "print(\"Step4. Result of pad_sequences:\")\n",
    "print(\"%s\\n\" % X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct LSTM\n",
    "----\n",
    "The code shown below constructs a LSTM with following structure:\n",
    "1. Embedding Layer:\n",
    "    * `input_dim`: `vocab_size`, which is initialized as 1024\n",
    "    * `output_dim`: `max_features`, which is initialized as 64\n",
    "    * `input_length`: `max_len`, which is initialized as 128\n",
    "2. LSTM Layer:\n",
    "    * Unit size 64\n",
    "3. Fully-connected layer\n",
    "    * Unit size 256\n",
    "4. Drop-out layer\n",
    "5. Fully-connected layer with sigmoid activation\n",
    "\n",
    "> **Your task**:  \n",
    "> Adjust the hierachy of your LSTM. Check Keras manual for more details. For examples:\n",
    "> - Embedding Layer: https://keras.io/layers/embeddings/#embedding\n",
    "> - LSTM Layer: https://keras.io/layers/recurrent/#lstm\n",
    "> - Dense Layer (Fully-connected layer): https://keras.io/layers/core/#dense\n",
    ">\n",
    "> You can also replace your `LSTM` layer with `SimpleRNN` or `GRU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Now it's your turn!: Adjust your LSTM\n",
    "    # ----------------------------------------------------------------\n",
    "    model.add(Embedding(vocab_size, max_features, input_length=max_len))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-validation\n",
    "----\n",
    "The code shown below conduct 5-fold cross-validation on training set based on the model adjusted above (i.e., `create_lstm`)\n",
    "\n",
    "> **Your task**:  \n",
    "> - Keep adjusting `create_lstm()` and execute the following block\n",
    "> - Make sure that you are happy with the 5-fold cross-validation result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_network = KerasClassifier(build_fn=create_lstm, epochs=5, batch_size=256, verbose=1)\n",
    "\n",
    "cv = 5\n",
    "scores = sklearn.model_selection.cross_val_score(neural_network, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"%d-fold Cross Validation Result\" % cv)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Predict the testing set\n",
    "----\n",
    "The code shown below helps you read the testing data, predict with your `create_lstm()`, then output as CSV files\n",
    "\n",
    "> **Your task:**\n",
    "> 1. Download testing set from Kaggle website:  https://www.kaggle.com/t/ff59441b7e064bc2a5d8e9374cfe1a11\n",
    "> 2. Put the testing set downloaded to the same location as this `*.ipynb` file\n",
    "> 3. Execute the following block to:\n",
    ">    - Train with `X_train` and `y_train`\n",
    ">    - Predict with `X_test`\n",
    "> 4. Upload your evaluation result to Kaggle (NOTICE: 5 submissions per-day!)\n",
    "> 5. Check your public scoreboard!\n",
    "> 6. Submit your homework 5 (Google form), make sure to **Fill in your Trend Micro PSID team name!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and predict the test data\n",
    "neural_network.fit(X_train, y_train)\n",
    "y_test_predict = neural_network.predict(X_test)\n",
    "\n",
    "# Output as CSV\n",
    "id_array = range(0,524)\n",
    "y_test_predict.astype('int8')\n",
    "submission = np.stack((id_array, y_test_predict.reshape((len(id_array)))), axis=1)\n",
    "np.savetxt(\"submission.csv\", submission, fmt=\"%d\", delimiter=',', header='id,answer', comments='')\n",
    "\n",
    "print(\"CONGRATULATIONS! YOU'VE ALREADY DONE! PLEASE SUBMIT YOUR submission.csv to https://www.kaggle.com/t/ff59441b7e064bc2a5d8e9374cfe1a11\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
